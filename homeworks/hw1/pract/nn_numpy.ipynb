{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooJtNr5dGrH9"
      },
      "source": [
        "**Name: Amir Mohammad Ezzati**\n",
        "\n",
        "**Student Number: 402212269**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJm9Z1k0cdmh"
      },
      "source": [
        "# Neural-Network with Numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDN075MYGesD"
      },
      "source": [
        "In this notebook, you are going to write and implement all the components required to create and train a two-layered neural network using NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt3FdxgNcdmm"
      },
      "source": [
        "## Imports & Seeding:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPZ4zlnxqhl5"
      },
      "source": [
        "Importing some common libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Et7OS7TGcdmn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "np.random.seed(42)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa2v2-xbcdmo"
      },
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKWqV2Gycdmp"
      },
      "source": [
        "You'll train and evaluate your model on [Fashion MNIST](https://en.wikipedia.org/wiki/Fashion_MNIST) dataset. In this section, you'll download Fashion MNIST and split it into training and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "tMYZtSoLc7c-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "742f50e2-508e-47ac-9477-b2b445b07b5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70000, 784) (70000,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Using `fetch_openml`, download `Fashion-MNIST`\n",
        "# and save the training data and labels in `X` and `y` respectively.\n",
        "#############################\n",
        "# Your code goes here (5 points)\n",
        "fashion_mnist = fetch_openml(name='Fashion-MNIST', version=1)\n",
        "\n",
        "X, y = fashion_mnist['data'], fashion_mnist['target']\n",
        "\n",
        "#############################\n",
        "\n",
        "# Normalization:\n",
        "X = ((X / 255.) - .5) * 2\n",
        "\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "sDmxyMJ4dBk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b503226c-901a-4a05-edff-34dce5926856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784) (60000,) (10000, 784) (10000,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Using `train_test_split`, split your data into two sets.\n",
        "# Set the test_size to 10000\n",
        "\n",
        "#############################\n",
        "# Your code goes here (6 points)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=42)\n",
        "\n",
        "#############################\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiGTXGXKcdmt"
      },
      "source": [
        "## Prepare training & validation sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba3nNYlDcdmt"
      },
      "source": [
        "We'll use only 3 classes from Fashion MNIST: Trouser, T-shirt, and Sneaker classes.\n",
        "\n",
        "The class labels for T-shirt, Trouser, and Sneaker are 0, 1, and 7 respectively.\n",
        "\n",
        "In this part, you'll limit the testing and training sets to only these three classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "TcBDZEtzcdmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70276a63-19e2-41fe-f306-ad4b3c14db40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18022, 784) (18022,)\n"
          ]
        }
      ],
      "source": [
        "# Modify `y_train` and `x_train`.\n",
        "# Only keep the 3 classes mentioned above.\n",
        "#############################\n",
        "# Your code goes here (4 points)\n",
        "\n",
        "def filter_classes(x, y):\n",
        "  selected_classes = [0, 1, 7]\n",
        "  y = y.astype(int)\n",
        "  mask = np.isin(y, selected_classes)\n",
        "\n",
        "  # Applying the mask to filter x_train and y_train\n",
        "  x_filtered = x[mask]\n",
        "  y_filtered = y[mask]\n",
        "\n",
        "  y_filtered[y_filtered == 7] = 2\n",
        "\n",
        "  return x_filtered, y_filtered\n",
        "\n",
        "x_train, y_train = filter_classes(x_train, y_train)\n",
        "#############################\n",
        "\n",
        "print(x_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "LX2hkRe1cdmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dff3d5f9-f9e0-40e2-ed0e-b59ea6b07594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2978, 784) (2978,)\n"
          ]
        }
      ],
      "source": [
        "# Modify `y_test` and `x_test`.\n",
        "# Only keep the 3 classes mentioned above.\n",
        "#############################\n",
        "# Your code goes here (4 points)\n",
        "\n",
        "x_test, y_test = filter_classes(x_test, y_test)\n",
        "\n",
        "#############################\n",
        "\n",
        "print(x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv6SMLUktWbv"
      },
      "source": [
        "## Linear & Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXlyJo5JteKC"
      },
      "source": [
        "In this part, you'll implement the forward and backward process for the following components:\n",
        "- Softmax Layer\n",
        "- Linear Layer\n",
        "- ReLU Layer\n",
        "- Sigmoid Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXtAD5uYA4sQ"
      },
      "source": [
        "### The `Softmax` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "tzaIVo-_Axp7"
      },
      "outputs": [],
      "source": [
        "class SoftMaxLayer(object):\n",
        "    def __init__(self):\n",
        "        self.inp = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Write the forward pass for softmax.\n",
        "        # Save the values required for the backward pass.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "\n",
        "        # Save input for backward pass\n",
        "        self.inp = x\n",
        "\n",
        "        x_shifted = x - np.max(x, axis=1, keepdims=True)\n",
        "        exp_x = np.exp(x_shifted)\n",
        "        self.output = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "        #############################\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, up_grad):\n",
        "        # Write the backward pass for softmax.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "\n",
        "        dx = np.zeros_like(self.output)\n",
        "\n",
        "        # Iterate over each sample in the batch\n",
        "        for i in range(self.output.shape[0]):\n",
        "            s = self.output[i].reshape(-1, 1)\n",
        "            jacobian = np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "            # dL/dx = (Jacobian of softmax) * dL/dy\n",
        "            dx[i] = np.dot(jacobian, up_grad[i])\n",
        "\n",
        "        return dx\n",
        "        #############################\n",
        "\n",
        "    def step(self, optimizer):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcFoIDZjcdnB"
      },
      "source": [
        "### The `Linear` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "1strsTh6cdnG"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        # Initialize the layer's weights and biases\n",
        "        #############################\n",
        "        # Your code goes here (2 points)\n",
        "\n",
        "        self.inp = None\n",
        "\n",
        "        self.w =  np.random.randn(in_dim, out_dim) * np.sqrt(2.0 / in_dim)  # He initialization\n",
        "        self.b = np.zeros((1, out_dim))  # bias initialized to zeros\n",
        "\n",
        "        #############################\n",
        "        self.dw = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # Compute linear layer's output.\n",
        "        # Save the value(s) required for the backward phase.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "\n",
        "        self.inp = inp\n",
        "\n",
        "        z = np.dot(inp, self.w) + self.b\n",
        "\n",
        "        #############################\n",
        "\n",
        "        return z\n",
        "\n",
        "    def backward(self, up_grad):\n",
        "        # Calculate the gradient with respect to the weights\n",
        "        # and biases and save the results.\n",
        "        #############################\n",
        "        # Your code goes here (6 points)\n",
        "\n",
        "        self.dw = np.dot(self.inp.T, up_grad)  # dL/dW = x^T * dL/dz\n",
        "        self.db = np.sum(up_grad, axis=0, keepdims=True)  # dL/db = sum(dL/dz)\n",
        "\n",
        "        down_grad = np.dot(up_grad, self.w.T)  # dL/dx = dL/dz * W^T\n",
        "\n",
        "        #############################\n",
        "        return down_grad\n",
        "\n",
        "    def step(self, optimizer):\n",
        "      # Update the layer's weights and biases\n",
        "      # Update previous_w_update and previous_b_update accordingly\n",
        "      #############################\n",
        "      # Your code goes here (5 points)\n",
        "\n",
        "      self.w = optimizer.get_next_update(self.w, self.dw)\n",
        "      self.b = optimizer.get_next_update(self.b, self.db)\n",
        "\n",
        "      #############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0Lfo-nhcdnG"
      },
      "source": [
        "### The `ReLU` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "tN6vcirMcdnH"
      },
      "outputs": [],
      "source": [
        "class RelU:\n",
        "    def __init__(self):\n",
        "        self.inp = None\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # Write the forward pass for ReLU.\n",
        "        # Save the value(s) required for the backward pass.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "\n",
        "        self.inp = inp\n",
        "        output = np.maximum(0, inp)\n",
        "\n",
        "        #############################\n",
        "        return output\n",
        "\n",
        "    def backward(self, up_grad):\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "\n",
        "        relu_grad = (self.inp > 0).astype(float)\n",
        "\n",
        "        down_grad = up_grad * relu_grad\n",
        "\n",
        "        #############################\n",
        "        return down_grad\n",
        "\n",
        "    def step(self, optimizer):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z00KoSI3cdnJ"
      },
      "source": [
        "### The `sigmoid` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "TTYYeL2lcdnJ"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "    def forward(self, inp):\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "\n",
        "        self.out = 1 / (1 + np.exp(-inp))\n",
        "\n",
        "        #############################\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, up_grad):\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "\n",
        "        sigmoid_grad = self.out * (1 - self.out)\n",
        "        down_grad = up_grad * sigmoid_grad\n",
        "\n",
        "        #############################\n",
        "        return down_grad\n",
        "\n",
        "    def step(self, optimizer):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zngleGY2cdnK"
      },
      "source": [
        "## `Loss` function :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISedT4FvcdnK"
      },
      "source": [
        "For this task we are going to use the [Cross-Entropy Loss](https://en.wikipedia.org/wiki/Cross_entropy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "XQyz4ybycdnL"
      },
      "outputs": [],
      "source": [
        "class CELoss():\n",
        "    def __init__(self):\n",
        "      pass\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "\n",
        "        self.yhat = pred\n",
        "        self.y = target\n",
        "        m = self.y.shape[0]\n",
        "        # Commpute and return the loss\n",
        "        #############################\n",
        "        # Your code goes here (8 points)\n",
        "\n",
        "        epsilon = 1e-10\n",
        "        self.yhat = np.clip(self.yhat, epsilon, 1 - epsilon)\n",
        "\n",
        "        # -1/m * sum(target * log(pred))\n",
        "        loss = -np.sum(self.y * np.log(self.yhat)) / m\n",
        "\n",
        "        return loss\n",
        "        #############################\n",
        "\n",
        "\n",
        "    def backward(self):\n",
        "        # Derivative of loss_fn with respect to a the predicted label.\n",
        "        # Use `self.y` and `self.yhat` to compute and return `grad`.\n",
        "        #############################\n",
        "        # Your code goes here (6 points)\n",
        "\n",
        "        m = self.y.shape[0]  # batch size\n",
        "\n",
        "        grad = (self.yhat - self.y) / m\n",
        "\n",
        "        #############################\n",
        "        return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xovZI-70kB9I"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "In this section, you'll implement an optimizer classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "h5ADTi5tkVTS"
      },
      "outputs": [],
      "source": [
        "class GradientDescent(object):\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def get_next_update(self, x, dx):\n",
        "        # Compute the new value for 'x' and return the result\n",
        "        #############################\n",
        "        # Your code goes here (2 points)\n",
        "\n",
        "        x_new = x - self.lr * dx\n",
        "\n",
        "        return x_new\n",
        "\n",
        "        #############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxxrEEovYEFi"
      },
      "source": [
        "## The Model\n",
        "Now you'll write the base class for a multi-layer perceptron network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "t8SoZeYRcdnY"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, layers, loss_fn, optimizer):\n",
        "        self.layers = layers\n",
        "        self.losses  = []\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # Pass `inp` to all the layers sequentially\n",
        "        # and return the result.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "\n",
        "        out = inp\n",
        "        for layer in self.layers:\n",
        "            out = layer.forward(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "        #############################\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        loss = self.loss_fn.forward(pred, label)\n",
        "        self.losses.append(loss)\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        # Start with loss function's gradient and\n",
        "        # do the backward pass on all the layers.\n",
        "        #############################\n",
        "        # Your code goes here (5 points)\n",
        "\n",
        "        grad = self.loss_fn.backward()\n",
        "        # Backpropagate through each layer in reverse order\n",
        "        for layer in reversed(self.layers):\n",
        "            grad = layer.backward(grad)\n",
        "\n",
        "        #############################\n",
        "\n",
        "    def update(self):\n",
        "        for layer in self.layers:\n",
        "          layer.step(self.optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo0rNwYciueF"
      },
      "source": [
        "The following cell encodes training labels into a one-hot representation with 3 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "nhJTulaFJ4vR"
      },
      "outputs": [],
      "source": [
        "def onehot_enc(y, num_labels):\n",
        "    ary = np.zeros((y.shape[0], num_labels))\n",
        "    for i, val in enumerate(y):\n",
        "        ary[i, val] = 1\n",
        "    return ary\n",
        "\n",
        "y_train = onehot_enc(y_train, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "TS6S_RUwsRkF"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, x, y):\n",
        "    for n in range(epochs):\n",
        "      # First do the forward pass. Next, compute the loss.\n",
        "      # Then do the backward pass and finally, update the parameters.\n",
        "      #############################\n",
        "      # Your code goes here (4 points)\n",
        "\n",
        "      pred = model.forward(x)\n",
        "\n",
        "      loss = model.loss(pred, y)\n",
        "\n",
        "      model.backward()  # Perform backpropagation\n",
        "\n",
        "      model.update()  # Update the model's parameters using the optimizer\n",
        "\n",
        "      #############################\n",
        "      print(f\"Loss at {n}: {loss:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "m1lSq2jNcdnY",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635d7a9a-c261-4e3f-fe4b-5011aa54dbbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at 0: 1.251\n",
            "Loss at 1: 1.234\n",
            "Loss at 2: 1.218\n",
            "Loss at 3: 1.202\n",
            "Loss at 4: 1.187\n",
            "Loss at 5: 1.172\n",
            "Loss at 6: 1.158\n",
            "Loss at 7: 1.143\n",
            "Loss at 8: 1.130\n",
            "Loss at 9: 1.117\n",
            "Loss at 10: 1.104\n",
            "Loss at 11: 1.091\n",
            "Loss at 12: 1.079\n",
            "Loss at 13: 1.068\n",
            "Loss at 14: 1.056\n",
            "Loss at 15: 1.045\n",
            "Loss at 16: 1.034\n",
            "Loss at 17: 1.024\n",
            "Loss at 18: 1.014\n",
            "Loss at 19: 1.004\n",
            "Loss at 20: 0.994\n",
            "Loss at 21: 0.985\n",
            "Loss at 22: 0.976\n",
            "Loss at 23: 0.967\n",
            "Loss at 24: 0.958\n",
            "Loss at 25: 0.949\n",
            "Loss at 26: 0.941\n",
            "Loss at 27: 0.932\n",
            "Loss at 28: 0.924\n",
            "Loss at 29: 0.916\n",
            "Loss at 30: 0.908\n",
            "Loss at 31: 0.901\n",
            "Loss at 32: 0.893\n",
            "Loss at 33: 0.886\n",
            "Loss at 34: 0.878\n",
            "Loss at 35: 0.871\n",
            "Loss at 36: 0.864\n",
            "Loss at 37: 0.857\n",
            "Loss at 38: 0.850\n",
            "Loss at 39: 0.843\n",
            "Loss at 40: 0.837\n",
            "Loss at 41: 0.830\n",
            "Loss at 42: 0.824\n",
            "Loss at 43: 0.817\n",
            "Loss at 44: 0.811\n",
            "Loss at 45: 0.805\n",
            "Loss at 46: 0.799\n",
            "Loss at 47: 0.792\n",
            "Loss at 48: 0.787\n",
            "Loss at 49: 0.781\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the `MLP` with the following structure:\n",
        "#     Linear with 50 units --> ReLU --> Linear with 50 units --> ReLU --> Linear with 3 units --> Sigmoid --> Softmax\n",
        "# Use GradientDescent as the optimizer, set the learning rate to 0.001, and use CELoss as the loss function.\n",
        "#############################\n",
        "# Your code goes here (4 points)\n",
        "\n",
        "layers = [\n",
        "    Linear(in_dim=x_train.shape[1], out_dim=50),  # Linear with 50 units\n",
        "    RelU(),                                       # ReLU\n",
        "    Linear(in_dim=50, out_dim=50),                # Linear with 50 units\n",
        "    RelU(),                                       # ReLU\n",
        "    Linear(in_dim=50, out_dim=3),                 # Linear with 3 units (for 3 classes)\n",
        "    Sigmoid(),                                    # Sigmoid\n",
        "    SoftMaxLayer()                                # Softmax\n",
        "]\n",
        "\n",
        "loss_fn = CELoss()\n",
        "optimizer = GradientDescent(lr=0.001)\n",
        "\n",
        "# MLP model\n",
        "nn = MLP(layers=layers, loss_fn=loss_fn, optimizer=optimizer)\n",
        "\n",
        "#############################\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "# Train the network using only `x_train` and `y_train` (no validation)\n",
        "train(nn, epochs, x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJec2xRJmY37"
      },
      "source": [
        "Let's plot the loss value for each iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "ymaQNn70cdnZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "bdc4b9da-f528-4433-fd4b-1542152cc7be"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(nn.losses)\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz3yqRa1cdna"
      },
      "source": [
        "**Let's also check our model's performance using the `accuracy` metric on the `testing` dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "TRqwXho7cdnd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "209446d5-517e-4269-f8ad-4458295cd9da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.716252518468771\n"
          ]
        }
      ],
      "source": [
        "# Compute the accuracy on the testing set\n",
        "#############################\n",
        "# Your code goes here (7 points)\n",
        "\n",
        "def compute_accuracy(model, x_test, y_test):\n",
        "    pred_probs = model.forward(x_test)\n",
        "    pred_labels = np.argmax(pred_probs, axis=1)  # Predicted class labels\n",
        "    accuracy = np.mean(pred_labels == y_test)  # Percentage of correct predictions\n",
        "    return accuracy\n",
        "\n",
        "acc = compute_accuracy(nn, x_test, y_test)\n",
        "\n",
        "#############################\n",
        "\n",
        "print(acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}