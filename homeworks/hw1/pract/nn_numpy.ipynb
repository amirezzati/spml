{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooJtNr5dGrH9"
   },
   "source": [
    "**Name:**\n",
    "\n",
    "**Student Number:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJm9Z1k0cdmh"
   },
   "source": [
    "# Neural-Network with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDN075MYGesD"
   },
   "source": [
    "In this notebook, you are going to write and implement all the components required to create and train a two-layered neural network using NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wt3FdxgNcdmm"
   },
   "source": [
    "## Imports & Seeding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPZ4zlnxqhl5"
   },
   "source": [
    "Importing some common libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Et7OS7TGcdmn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fa2v2-xbcdmo"
   },
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKWqV2Gycdmp"
   },
   "source": [
    "You'll train and evaluate your model on [Fashion MNIST](https://en.wikipedia.org/wiki/Fashion_MNIST) dataset. In this section, you'll download Fashion MNIST and split it into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMYZtSoLc7c-"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Using `fetch_openml`, download `Fashion-MNIST` \n",
    "# and save the training data and labels in `X` and `y` respectively.\n",
    "#############################\n",
    "# Your code goes here (5 points)\n",
    "\n",
    "#############################\n",
    "\n",
    "# Normalization:\n",
    "X = ((X / 255.) - .5) * 2\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDmxyMJ4dBk3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using `train_test_split`, split your data into two sets. \n",
    "# Set the test_size to 10000\n",
    "\n",
    "#############################\n",
    "# Your code goes here (6 points)\n",
    "\n",
    "#############################\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiGTXGXKcdmt"
   },
   "source": [
    "## Prepare training & validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba3nNYlDcdmt"
   },
   "source": [
    "We'll use only 3 classes from Fashion MNIST: Trouser, T-shirt, and Sneaker classes.\n",
    "\n",
    "The class labels for T-shirt, Trouser, and Sneaker are 0, 1, and 7 respectively.\n",
    "\n",
    "In this part, you'll limit the testing and training sets to only these three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcBDZEtzcdmu"
   },
   "outputs": [],
   "source": [
    "# Modify `y_train` and `x_train`.\n",
    "# Only keep the 3 classes mentioned above. \n",
    "#############################\n",
    "# Your code goes here (4 points)\n",
    "\n",
    "#############################\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LX2hkRe1cdmw"
   },
   "outputs": [],
   "source": [
    "# Modify `y_test` and `x_test`.\n",
    "# Only keep the 3 classes mentioned above. \n",
    "#############################\n",
    "# Your code goes here (4 points)\n",
    "\n",
    "#############################\n",
    "\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv6SMLUktWbv"
   },
   "source": [
    "## Linear & Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXlyJo5JteKC"
   },
   "source": [
    "In this part, you'll implement the forward and backward process for the following components:\n",
    "- Softmax Layer\n",
    "- Linear Layer\n",
    "- ReLU Layer\n",
    "- Sigmoid Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXtAD5uYA4sQ"
   },
   "source": [
    "### The `Softmax` Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzaIVo-_Axp7"
   },
   "outputs": [],
   "source": [
    "class SoftMaxLayer(object):\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Write the forward pass for softmax.\n",
    "        # Save the values required for the backward pass.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "\n",
    "        #############################\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, up_grad):\n",
    "        # Write the backward pass for softmax.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "\n",
    "        #############################\n",
    "\n",
    "    def step(self, optimizer):\n",
    "      pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcFoIDZjcdnB"
   },
   "source": [
    "### The `Linear` Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1strsTh6cdnG"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        # Initialize the layer's weights and biases\n",
    "        #############################\n",
    "        # Your code goes here (2 points)\n",
    "        \n",
    "        #############################\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        # Compute linear layer's output.\n",
    "        # Save the value(s) required for the backward phase.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "\n",
    "        #############################\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def backward(self, up_grad):\n",
    "        # Calculate the gradient with respect to the weights \n",
    "        # and biases and save the results.\n",
    "        #############################\n",
    "        # Your code goes here (6 points)\n",
    "\n",
    "        #############################\n",
    "        return down_grad\n",
    "\n",
    "    def step(self, optimizer):\n",
    "      # Update the layer's weights and biases\n",
    "      # Update previous_w_update and previous_b_update accordingly\n",
    "      #############################\n",
    "      # Your code goes here (5 points)\n",
    "\n",
    "      #############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0Lfo-nhcdnG"
   },
   "source": [
    "### The `ReLU` Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tN6vcirMcdnH"
   },
   "outputs": [],
   "source": [
    "class RelU:\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Write the forward pass for ReLU.\n",
    "        # Save the value(s) required for the backward pass.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "\n",
    "        #############################\n",
    "        return output\n",
    "    \n",
    "    def backward(self, up_grad):\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "\n",
    "        #############################\n",
    "        return down_grad\n",
    "\n",
    "    def step(self, optimizer):\n",
    "      pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z00KoSI3cdnJ"
   },
   "source": [
    "### The `sigmoid` Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTYYeL2lcdnJ"
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self, inp):\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "\n",
    "        #############################\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, up_grad):\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "\n",
    "        #############################\n",
    "        return down_grad\n",
    "    \n",
    "    def step(self, optimizer):\n",
    "      pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zngleGY2cdnK"
   },
   "source": [
    "## `Loss` function :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISedT4FvcdnK"
   },
   "source": [
    "For this task we are going to use the [Cross-Entropy Loss](https://en.wikipedia.org/wiki/Cross_entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQyz4ybycdnL"
   },
   "outputs": [],
   "source": [
    "class CELoss():\n",
    "    def __init__(self):\n",
    "      pass\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        \n",
    "        self.yhat = pred\n",
    "        self.y = target\n",
    "        m = self.y.shape[0]\n",
    "        # Commpute and return the loss \n",
    "        #############################\n",
    "        # Your code goes here (8 points)\n",
    "        \n",
    "        #############################\n",
    "        \n",
    "    \n",
    "    def backward(self):\n",
    "        # Derivative of loss_fn with respect to a the predicted label.\n",
    "        # Use `self.y` and `self.yhat` to compute and return `grad`.\n",
    "        #############################\n",
    "        # Your code goes here (6 points)\n",
    "        \n",
    "        #############################\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xovZI-70kB9I"
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "In this section, you'll implement an optimizer classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5ADTi5tkVTS"
   },
   "outputs": [],
   "source": [
    "class GradientDescent(object):\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def get_next_update(self, x, dx):\n",
    "        # Compute the new value for 'x' and return the result\n",
    "        #############################\n",
    "        # Your code goes here (2 points)\n",
    "\n",
    "        #############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxxrEEovYEFi"
   },
   "source": [
    "## The Model\n",
    "Now you'll write the base class for a multi-layer perceptron network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8SoZeYRcdnY"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layers, loss_fn, optimizer):\n",
    "        self.layers = layers \n",
    "        self.losses  = [] \n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Pass `inp` to all the layers sequentially\n",
    "        # and return the result.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "\n",
    "        #############################\n",
    "        \n",
    "    def loss(self, pred, label):\n",
    "        loss = self.loss_fn.forward(pred, label)\n",
    "        self.losses.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        # Start with loss function's gradient and \n",
    "        # do the backward pass on all the layers.\n",
    "        #############################\n",
    "        # Your code goes here (5 points)\n",
    "\n",
    "        #############################\n",
    "        \n",
    "    def update(self):\n",
    "        for layer in self.layers:\n",
    "          layer.step(self.optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zo0rNwYciueF"
   },
   "source": [
    "The following cell encodes training labels into a one-hot representation with 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhJTulaFJ4vR"
   },
   "outputs": [],
   "source": [
    "def onehot_enc(y, num_labels):\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for i, val in enumerate(y):\n",
    "        ary[i, val] = 1\n",
    "    return ary\n",
    "\n",
    "y_train = onehot_enc(y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TS6S_RUwsRkF"
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, x, y):\n",
    "    for n in range(epochs):\n",
    "      # First do the forward pass. Next, compute the loss.\n",
    "      # Then do the backward pass and finally, update the parameters.\n",
    "      #############################\n",
    "      # Your code goes here (4 points)\n",
    "\n",
    "      #############################\n",
    "      print(f\"Loss at {n}: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1lSq2jNcdnY",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instantiate the `MLP` with the following structure:\n",
    "#     Linear with 50 units --> ReLU --> Linear with 50 units --> ReLU --> Linear with 3 units --> Sigmoid --> Softmax\n",
    "# Use GradientDescent as the optimizer, set the learning rate to 0.001, and use CELoss as the loss function.\n",
    "#############################\n",
    "# Your code goes here (4 points)\n",
    "\n",
    "#############################\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "# Train the network using only `x_train` and `y_train` (no validation)\n",
    "train(nn, epochs, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJec2xRJmY37"
   },
   "source": [
    "Let's plot the loss value for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymaQNn70cdnZ"
   },
   "outputs": [],
   "source": [
    "plt.plot(nn.losses)\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dz3yqRa1cdna"
   },
   "source": [
    "**Let's also check our model's performance using the `accuracy` metric on the `testing` dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRqwXho7cdnd"
   },
   "outputs": [],
   "source": [
    "# Compute the accuracy on the testing set\n",
    "#############################\n",
    "# Your code goes here (7 points)\n",
    "\n",
    "#############################\n",
    "\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
